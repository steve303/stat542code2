# X: n-by-p design matrix without the intercept
# y: n-by-1 response vector
# lam.seq: sequence of lambda values
# maxit: number of updates for each lambda
# Center/Scale X
# Center y
n = length(y)
p = dim(X)[2]
nlam = length(lam.seq)
##############################
# YOUR CODE:
# Record the corresponding means and scales
# For example,
# y.mean = mean(y)
# Xs = centered and scaled X
##############################
Xs0 = X
col_means = colMeans(Xs0)
col_stdev = apply(Xs0, 2, sd)
means_m = matrix(rep(col_means, n), byrow = TRUE, nrow = n)
stdev_m = matrix(rep(col_stdev, n), byrow = TRUE, nrow = n)
means_m <<- means_m
stdev_m <<- stdev_m
Xs0 = (Xs0 - means_m)/stdev_m
Xs = Xs0
check_stdev = apply(Xs, 2, sd)
check_stdev <<- check_stdev
y.mean = mean(y)
# Initilize coef vector b and residual vector r
b = rep(0, p)
r = y
B = matrix(nrow = nlam, ncol = p + 1)
# Triple nested loop
for (m in 1:nlam) {
lam = 2 * n * lam.seq[m]
for (step in 1:maxit) {
for (j in 1:p) {              #j refers to one of the predictors, p
r = r + (Xs[, j] * b[j])
b[j] = one_var_lasso(r, Xs[, j], lam)
r = r - Xs[, j] * b[j]
}
}
B[m, ] = c(0, b)  #here we concat 0 as a place holder for intercept
}
##############################
# YOUR CODE:
# Scale back the coefficients;
# Update the intercepts stored in B[, 1]
##############################
B <<- B
tmp_stdev_matrix =  matrix(rep(col_stdev, nlam), byrow = TRUE, nrow = nlam)
tmp_mean_matrix = matrix(rep(col_means, nlam), byrow = TRUE, nrow = nlam)
tmp_stdev_matrix <<- tmp_stdev_matrix
tmp_mean_matrix <<- tmp_mean_matrix
B_std = B / cbind(rep(1, nlam), tmp_stdev_matrix)  #need to take out col1 for the intercept
temp = (B_std * cbind(rep(1, nlam), tmp_mean_matrix) * -1)
#should be + or - ???
A = y.mean + apply(temp, 1, sum) #A is a vector containg all intercepts for each lambda, length = nlam
B_std[ ,1] = A  #update the intercept into the standardized B matrix, "B_std"
return(B_std)
}
one_var_lasso = function(r, x, lam) {
xx = sum(x^2)
xr = sum(r * x)
b = (abs(xr) - lam/2)/xx
b = sign(xr) * ifelse(b > 0, b, 0)
return(b)
}
lam.seq = exp(seq(-1, -8, length.out = 80))
myout = MyLasso(X, y, lam.seq, maxit = 100)
colnames(myout) = c("Intercept", colnames(X))
dim(myout)
x.index = log(lam.seq)
beta = myout[,-1 ]  # beta is a 13-by-80 matrix
matplot(x.index, beta,
xlim = c(min(x.index), max(x.index)),
lty = 1,
xlab = "Log Lambda",
ylab = "Coefficients",
type="l",
lwd = 1)
# You can add variable names to each path
# var.names = colnames(X)
# nvar = length(var.names)
# xpos = rep(min(x.index), nvar)
# ypos = beta[, ncol(beta)]
# text(xpos, ypos, var.names, cex=0.5, pos=2)
library(glmnet)
lasso.fit = glmnet(X, y, alpha = 1, lambda = lam.seq)
coef_lasso = t(as.matrix(coef(lasso.fit)))
write.csv(as.matrix(coef(lasso.fit)), file = "Coding2_lasso_coefs.csv",
row.names = FALSE)
#max(abs(coef(lasso.fit) - myout))
max(abs(coef_lasso - myout))
plot(lasso.fit, xvar = "lambda")
library(glmnet)
library(pls)
set.seed(1093)
myData = read.csv("BostonData2.csv")
myData = myData[, -1]
dim(myData)
#names(myData)
X = data.matrix(myData[,-1])
Y = myData[,1]
T = 50
n = length(Y)
ntest = round(n * 0.25)  # test set size
ntrain = n - ntest  # training set size
all.test.id = matrix(0, ntest, T)  #
for(t in 1:T){
all.test.id[, t] = sample(1:n, ntest)
}
#save(all.test.id, file="alltestID.RData")
test.id = all.test.id[,1]
MSPE = rep(0, 7)
names(MSPE) = c("Full", "R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
full.model = lm(Y ~ ., data = myData[-test.id,])
Ytest.pred = predict(full.model, newdata = myData[test.id,])
MSPE[1] = mean((myData$Y[test.id] - Ytest.pred)^2)
full_model = function(data, test.id){
full.model = lm(Y ~ ., data = myData[-test.id,])
Ytest.pred = predict(full.model, newdata = myData[test.id,])
mean((myData$Y[test.id] - Ytest.pred)^2)
}
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[2] = mean((Y[test.id] - Ytest.pred)^2)
sum(cv.out$lambda < best.lam)
plot(cv.out)
mylasso.lambda.seq = exp(seq(-4, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0,
lambda = mylasso.lambda.seq)
plot(cv.out)
mylasso.lambda.seq = exp(seq(-10, -2, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[2] = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[3] = mean((Y[test.id] - Ytest.pred)^2)
ridge_regression = function(X, Y, lambda.seq, test.id){
mylasso.lambda.seq = lambda.seq
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
R_min = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
R_1se = mean((Y[test.id] - Ytest.pred)^2)
plot(cv.out)
return(c(R_min, R_1se))
}
test.id = all.test.id[,2]
test.lambda.seq = exp(seq(-12, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = test.lambda.seq)
sum(cv.out$lambda < cv.out$lambda.min)
test.id = all.test.id[,9]
test.lambda.seq = exp(seq(-12, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = test.lambda.seq)
plot(cv.out)
ridge.lambda.seq = test.lambda.seq
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[4] = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[5] = mean((Y[test.id] - Ytest.pred)^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
MSPE[6] = mean((Ytest.pred - Y[test.id])^2)
plot(cv.out)
lasso_regression = function(X, Y, test.id){
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
L_min = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
L_1se = mean((Y[test.id] - Ytest.pred)^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
L_Refit = mean((Ytest.pred - Y[test.id])^2)
return(c(L_min, L_1se, L_Refit))
}
mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV")
CVerr = RMSEP(mypcr)$val[1, , ]
adjCVerr = RMSEP(mypcr)$val[2, , ]
best.ncomp = which.min(CVerr) - 1
best.ncomp
if (best.ncomp==0) {
Ytest.pred = mean(myData$Y[-test.id])
} else {
Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
}
MSPE[7] = mean((Ytest.pred - myData$Y[test.id])^2)
princomp_regression = function(myData, test.id){
mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV")
CVerr = RMSEP(mypcr)$val[1, , ]
adjCVerr = RMSEP(mypcr)$val[2, , ]
best.ncomp = which.min(CVerr) - 1
if (best.ncomp==0) {
Ytest.pred = mean(myData$Y[-test.id])
} else {
Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
}
return(mean((Ytest.pred - myData$Y[test.id])^2))
}
MSPE
MSPE = matrix(nrow = T, ncol = 7)
colnames(MSPE) = c("Full", "R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
for(t in 1:T){
test.id = all.test.id[,t]
MSPE[t, 1] = full_model(myData, test.id)
MSPE[t, 2:3] = ridge_regression(X, Y, ridge.lambda.seq, test.id)
MSPE[t, 4:6] = lasso_regression(X, Y, test.id)
MSPE[t, 7] = princomp_regression(myData, test.id)
}
boxplot(MSPE)
myData = read.csv("BostonData3.csv")
myData = myData[, -1]
dim(myData)
X = data.matrix(myData[,-1])
Y = myData[,1]
MSPE = matrix(nrow = T, ncol = 6)
colnames(MSPE) = c("R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
for(t in 1:T){
test.id = all.test.id[,t]
MSPE[t, 1:2] = ridge_regression(X, Y, ridge.lambda.seq, test.id)
MSPE[t, 3:5] = lasso_regression(X, Y, test.id)
MSPE[t, 6] = princomp_regression(myData, test.id)
}
ridge_regression = function(X, Y, lambda.seq, test.id){
mylasso.lambda.seq = lambda.seq
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
R_min = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
R_1se = mean((Y[test.id] - Ytest.pred)^2)
return(c(R_min, R_1se))
}
library(MASS)
myData = Boston
names(myData)[14] = "Y"
iLog = c(1, 3, 5, 6, 8, 9, 10, 14);
myData[, iLog] = log(myData[, iLog]);
myData[, 2] = myData[, 2] / 10;
myData[, 7] = myData[, 7]^2.5 / 10^4
myData[, 11] = exp(0.4 * myData[, 11]) / 1000;
myData[, 12] = myData[, 12] / 100;
myData[, 13] = sqrt(myData[, 13]);
write.csv(myData, file = "Coding2_myData.csv",
row.names = FALSE)
# After preparing the dataset, remove all objects and the attached MASS library
rm(list=objects())
detach("package:MASS")
myData = read.csv("Coding2_myData.csv")
X = as.matrix(myData[, -14])
y = myData$Y
dim(X)
MyLasso = function(X, y, lam.seq, maxit = 500) {
# X: n-by-p design matrix without the intercept
# y: n-by-1 response vector
# lam.seq: sequence of lambda values
# maxit: number of updates for each lambda
# Center/Scale X
# Center y
n = length(y)
p = dim(X)[2]
nlam = length(lam.seq)
##############################
# YOUR CODE:
# Record the corresponding means and scales
# For example,
# y.mean = mean(y)
# Xs = centered and scaled X
##############################
Xs0 = X
col_means = colMeans(Xs0)
col_stdev = apply(Xs0, 2, sd)
means_m = matrix(rep(col_means, n), byrow = TRUE, nrow = n)
stdev_m = matrix(rep(col_stdev, n), byrow = TRUE, nrow = n)
means_m <<- means_m
stdev_m <<- stdev_m
Xs0 = (Xs0 - means_m)/stdev_m
Xs = Xs0
check_stdev = apply(Xs, 2, sd)
check_stdev <<- check_stdev
y.mean = mean(y)
# Initilize coef vector b and residual vector r
b = rep(0, p)
r = y
B = matrix(nrow = nlam, ncol = p + 1)
# Triple nested loop
for (m in 1:nlam) {
lam = 2 * n * lam.seq[m]
for (step in 1:maxit) {
for (j in 1:p) {              #j refers to one of the predictors, p
r = r + (Xs[, j] * b[j])
b[j] = one_var_lasso(r, Xs[, j], lam)
r = r - Xs[, j] * b[j]
}
}
B[m, ] = c(0, b)  #here we concat 0 as a place holder for intercept
}
##############################
# YOUR CODE:
# Scale back the coefficients;
# Update the intercepts stored in B[, 1]
##############################
B <<- B
tmp_stdev_matrix =  matrix(rep(col_stdev, nlam), byrow = TRUE, nrow = nlam)
tmp_mean_matrix = matrix(rep(col_means, nlam), byrow = TRUE, nrow = nlam)
tmp_stdev_matrix <<- tmp_stdev_matrix
tmp_mean_matrix <<- tmp_mean_matrix
B_std = B / cbind(rep(1, nlam), tmp_stdev_matrix)  #need to take out col1 for the intercept
temp = (B_std * cbind(rep(1, nlam), tmp_mean_matrix) * -1)
#should be + or - ???
A = y.mean + apply(temp, 1, sum) #A is a vector containg all intercepts for each lambda, length = nlam
B_std[ ,1] = A  #update the intercept into the standardized B matrix, "B_std"
return(B_std)
}
one_var_lasso = function(r, x, lam) {
xx = sum(x^2)
xr = sum(r * x)
b = (abs(xr) - lam/2)/xx
b = sign(xr) * ifelse(b > 0, b, 0)
return(b)
}
lam.seq = exp(seq(-1, -8, length.out = 80))
myout = MyLasso(X, y, lam.seq, maxit = 100)
colnames(myout) = c("Intercept", colnames(X))
dim(myout)
x.index = log(lam.seq)
beta = myout[,-1 ]  # beta is a 13-by-80 matrix
matplot(x.index, beta,
xlim = c(min(x.index), max(x.index)),
lty = 1,
xlab = "Log Lambda",
ylab = "Coefficients",
type="l",
lwd = 1)
# You can add variable names to each path
# var.names = colnames(X)
# nvar = length(var.names)
# xpos = rep(min(x.index), nvar)
# ypos = beta[, ncol(beta)]
# text(xpos, ypos, var.names, cex=0.5, pos=2)
library(glmnet)
lasso.fit = glmnet(X, y, alpha = 1, lambda = lam.seq)
coef_lasso = t(as.matrix(coef(lasso.fit)))
write.csv(as.matrix(coef(lasso.fit)), file = "Coding2_lasso_coefs.csv",
row.names = FALSE)
#max(abs(coef(lasso.fit) - myout))
max(abs(coef_lasso - myout))
plot(lasso.fit, xvar = "lambda")
library(glmnet)
library(pls)
set.seed(1093)
myData = read.csv("BostonData2.csv")
myData = myData[, -1]
dim(myData)
#names(myData)
X = data.matrix(myData[,-1])
Y = myData[,1]
T = 50
n = length(Y)
ntest = round(n * 0.25)  # test set size
ntrain = n - ntest  # training set size
all.test.id = matrix(0, ntest, T)  #
for(t in 1:T){
all.test.id[, t] = sample(1:n, ntest)
}
#save(all.test.id, file="alltestID.RData")
test.id = all.test.id[,1]
MSPE = rep(0, 7)
names(MSPE) = c("Full", "R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
full.model = lm(Y ~ ., data = myData[-test.id,])
Ytest.pred = predict(full.model, newdata = myData[test.id,])
MSPE[1] = mean((myData$Y[test.id] - Ytest.pred)^2)
full_model = function(data, test.id){
full.model = lm(Y ~ ., data = myData[-test.id,])
Ytest.pred = predict(full.model, newdata = myData[test.id,])
mean((myData$Y[test.id] - Ytest.pred)^2)
}
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[2] = mean((Y[test.id] - Ytest.pred)^2)
sum(cv.out$lambda < best.lam)
plot(cv.out)
mylasso.lambda.seq = exp(seq(-4, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0,
lambda = mylasso.lambda.seq)
plot(cv.out)
mylasso.lambda.seq = exp(seq(-10, -2, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[2] = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[3] = mean((Y[test.id] - Ytest.pred)^2)
ridge_regression = function(X, Y, lambda.seq, test.id){
mylasso.lambda.seq = lambda.seq
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0,
lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
R_min = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
R_1se = mean((Y[test.id] - Ytest.pred)^2)
return(c(R_min, R_1se))
}
test.id = all.test.id[,2]
test.lambda.seq = exp(seq(-12, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = test.lambda.seq)
sum(cv.out$lambda < cv.out$lambda.min)
test.id = all.test.id[,9]
test.lambda.seq = exp(seq(-12, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = test.lambda.seq)
plot(cv.out)
ridge.lambda.seq = test.lambda.seq
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[4] = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[5] = mean((Y[test.id] - Ytest.pred)^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
MSPE[6] = mean((Ytest.pred - Y[test.id])^2)
plot(cv.out)
lasso_regression = function(X, Y, test.id){
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
L_min = mean((Y[test.id] - Ytest.pred)^2)
best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
L_1se = mean((Y[test.id] - Ytest.pred)^2)
mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
L_Refit = mean((Ytest.pred - Y[test.id])^2)
return(c(L_min, L_1se, L_Refit))
}
mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV")
CVerr = RMSEP(mypcr)$val[1, , ]
adjCVerr = RMSEP(mypcr)$val[2, , ]
best.ncomp = which.min(CVerr) - 1
best.ncomp
if (best.ncomp==0) {
Ytest.pred = mean(myData$Y[-test.id])
} else {
Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
}
MSPE[7] = mean((Ytest.pred - myData$Y[test.id])^2)
princomp_regression = function(myData, test.id){
mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV")
CVerr = RMSEP(mypcr)$val[1, , ]
adjCVerr = RMSEP(mypcr)$val[2, , ]
best.ncomp = which.min(CVerr) - 1
if (best.ncomp==0) {
Ytest.pred = mean(myData$Y[-test.id])
} else {
Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
}
return(mean((Ytest.pred - myData$Y[test.id])^2))
}
MSPE
MSPE = matrix(nrow = T, ncol = 7)
colnames(MSPE) = c("Full", "R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
for(t in 1:T){
test.id = all.test.id[,t]
MSPE[t, 1] = full_model(myData, test.id)
MSPE[t, 2:3] = ridge_regression(X, Y, ridge.lambda.seq, test.id)
MSPE[t, 4:6] = lasso_regression(X, Y, test.id)
MSPE[t, 7] = princomp_regression(myData, test.id)
}
boxplot(MSPE)
myData = read.csv("BostonData3.csv")
myData = myData[, -1]
dim(myData)
X = data.matrix(myData[,-1])
Y = myData[,1]
MSPE = matrix(nrow = T, ncol = 6)
colnames(MSPE) = c("R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
for(t in 1:T){
test.id = all.test.id[,t]
MSPE[t, 1:2] = ridge_regression(X, Y, ridge.lambda.seq, test.id)
MSPE[t, 3:5] = lasso_regression(X, Y, test.id)
MSPE[t, 6] = princomp_regression(myData, test.id)
}
boxplot(MSPE)

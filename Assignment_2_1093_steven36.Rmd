---
title: "PSL (S21) Coding Assignment 2"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
  html_notebook:
    theme: readable
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "02/05/2021"
---
# Part I
## Load Data

You can ignore the code snippet below and start your code with loading "Coding2_myData.csv"

```{r eval=TRUE}
library(MASS)
myData = Boston
names(myData)[14] = "Y"
iLog = c(1, 3, 5, 6, 8, 9, 10, 14);
myData[, iLog] = log(myData[, iLog]);
myData[, 2] = myData[, 2] / 10;
myData[, 7] = myData[, 7]^2.5 / 10^4
myData[, 11] = exp(0.4 * myData[, 11]) / 1000;
myData[, 12] = myData[, 12] / 100;
myData[, 13] = sqrt(myData[, 13]);
write.csv(myData, file = "Coding2_myData.csv", 
          row.names = FALSE)

# After preparing the dataset, remove all objects and the attached MASS library
rm(list=objects())  
detach("package:MASS")
```


**You can start here.**

```{r}
myData = read.csv("Coding2_myData.csv")
X = as.matrix(myData[, -14])
y = myData$Y
dim(X)
```


## CD for Lasso 

Implement the Coordinate Descent algorithm for Lasso. Some part of the function `MyLasso` is blocked here, but your submission should include all code used to produce your results. 

```{r eval=TRUE}
MyLasso = function(X, y, lam.seq, maxit = 500) {
    
    # X: n-by-p design matrix without the intercept 
    # y: n-by-1 response vector 
    # lam.seq: sequence of lambda values 
    # maxit: number of updates for each lambda 
    # Center/Scale X
    # Center y
  
    n = length(y)
    p = dim(X)[2]
    nlam = length(lam.seq)
  
    ##############################
    # YOUR CODE: 
    # Record the corresponding means and scales
    # For example, 
    # y.mean = mean(y)
    # Xs = centered and scaled X
    ##############################
    
    Xs0 = X
    col_means = colMeans(Xs0)
    col_stdev = apply(Xs0, 2, sd)
    means_m = matrix(rep(col_means, n), byrow = TRUE, nrow = n)
    stdev_m = matrix(rep(col_stdev, n), byrow = TRUE, nrow = n)
    means_m <<- means_m
    stdev_m <<- stdev_m
    Xs0 = (Xs0 - means_m)/stdev_m  
    Xs = Xs0
    
    check_stdev = apply(Xs, 2, sd)
    check_stdev <<- check_stdev
    y.mean = mean(y)   
    # Initilize coef vector b and residual vector r
    b = rep(0, p)
    r = y
    B = matrix(nrow = nlam, ncol = p + 1)
    
    # Triple nested loop
    for (m in 1:nlam) {
        lam = 2 * n * lam.seq[m]
        for (step in 1:maxit) {
            for (j in 1:p) {              #j refers to one of the predictors, p
                r = r + (Xs[, j] * b[j])
                b[j] = one_var_lasso(r, Xs[, j], lam)
                r = r - Xs[, j] * b[j]
            }
        }
        B[m, ] = c(0, b)  #here we concat 0 as a place holder for intercept
    }
   
    ##############################
    # YOUR CODE:
    # Scale back the coefficients;
    # Update the intercepts stored in B[, 1]
    ##############################
    B <<- B
    tmp_stdev_matrix =  matrix(rep(col_stdev, nlam), byrow = TRUE, nrow = nlam)
    tmp_mean_matrix = matrix(rep(col_means, nlam), byrow = TRUE, nrow = nlam)
    tmp_stdev_matrix <<- tmp_stdev_matrix
    tmp_mean_matrix <<- tmp_mean_matrix
    B_std = B / cbind(rep(1, nlam), tmp_stdev_matrix)  #need to take out col1 for the intercept
    temp = (B_std * cbind(rep(1, nlam), tmp_mean_matrix) * -1)
    #should be + or - ???
    A = y.mean + apply(temp, 1, sum) #A is a vector containg all intercepts for each lambda, length = nlam
    B_std[ ,1] = A  #update the intercept into the standardized B matrix, "B_std"
    
    return(B_std)
}
```


## Some Technical Details

* We use the following function to solve the Lasso estimate for $\beta_j$ given other coefficients fixed; see the derivation in Coding2.pdf. 

```{r}
one_var_lasso = function(r, x, lam) {
    xx = sum(x^2)
    xr = sum(r * x)
    b = (abs(xr) - lam/2)/xx
    b = sign(xr) * ifelse(b > 0, b, 0)
    return(b)
}
```

* `glmnet` standardizes the data using a different definition of "standard deviation", which is divided by <b>n</b>, while the command `sd(z)` in R divides the sum of squares by <b>(n-1)</b> where z is a n-by-1 data vector. We suggest to use `sd(z) * sqrt((n-1)/n)` to compute the standard deviation of a data vetor used in `myLasso`, since
$$
\sqrt{\frac{\sum_i (z_i - \text{z.mean})^2}{n}} = \sqrt{\frac{\sum_i (z_i - \text{z.mean})^2}{n-1}} \cdot \sqrt{\frac{n-1}{n}} = \text{sd(z)} \cdot \sqrt{\frac{n-1}{n}}.
$$


* Why we need to scale the lambda value by (2n) in `lam = 2*n*lam.seq[m]`? As detailed in Coding2.pdf, the `one_var_lasso` function is derived based on objective function
$$\text{RSS} + \lambda \cdot | \beta|, $$
while the objective function used in `glmnet` is
$$
\frac{1}{2n} \text{RSS} + \lambda | \beta| \ \propto \ \text{RSS} + 2 n \lambda | \cdot \beta| 
$$
So to compare results from the two algorithms on an equal footing, we need to scale our lambda value by (2n). 



## Test Your Code

Test your code with the following lambda sequence. The 80 sets of coefficients (including the intercept) are stored in a 14-by-80 matrix `myout`. 

```{r}
lam.seq = exp(seq(-1, -8, length.out = 80))
myout = MyLasso(X, y, lam.seq, maxit = 100) 
colnames(myout) = c("Intercept", colnames(X)) 
dim(myout)
```

Produce the path plot for the 13 non-intercept coefficients with the x-coordinate to be the lambda values in log-scale. 

```{r}
x.index = log(lam.seq)
beta = myout[,-1 ]  # beta is a 13-by-80 matrix
matplot(x.index, beta,
        xlim = c(min(x.index), max(x.index)),
        lty = 1,
        xlab = "Log Lambda",
        ylab = "Coefficients",
        type="l", 
        lwd = 1)

# You can add variable names to each path
# var.names = colnames(X)
# nvar = length(var.names)
# xpos = rep(min(x.index), nvar)
# ypos = beta[, ncol(beta)]
# text(xpos, ypos, var.names, cex=0.5, pos=2)
```


## Check the Accuracy 

Compare  the accuracy of your algorithm against the output from `glmnet`. The maximum difference between the two coefficient matrices should be <font color="red">less than 0.005</font>.

```{r}
library(glmnet)
lasso.fit = glmnet(X, y, alpha = 1, lambda = lam.seq)
coef_lasso = t(as.matrix(coef(lasso.fit)))
write.csv(as.matrix(coef(lasso.fit)), file = "Coding2_lasso_coefs.csv", 
          row.names = FALSE)

```

```{r}
#max(abs(coef(lasso.fit) - myout))
max(abs(coef_lasso - myout))
```

Your plot should look the plot from `glmnet`

```{r}
plot(lasso.fit, xvar = "lambda")
```

# Part II
```{r}
library(glmnet) 
library(pls)
set.seed(1093)
```

## Data Preparation

```{r}
myData = read.csv("BostonData2.csv")
myData = myData[, -1]
dim(myData)
#names(myData)
```

Some algorithms need the matrix/vector input (instead of a data frame)
```{r}
X = data.matrix(myData[,-1])  
Y = myData[,1]  
```

We will repeat the following simulation 50 times. In each iteration, randomly split the data into two parts, 75\% for training and 25\% for testing. 

You can write a loop with 50 iterations, and in each iteration, split the data and run the seven procedures (six procedures for `BostonData3.csv`). 

Or you can save the row IDs for the 50 test datasets, then write a separate loop for each method. Using `all.test.id` (produced below), you can ensure that you use the same training/test split for each procedure.


```{r}
T = 50
n = length(Y)
ntest = round(n * 0.25)  # test set size
ntrain = n - ntest  # training set size
all.test.id = matrix(0, ntest, T)  # 
for(t in 1:T){
  all.test.id[, t] = sample(1:n, ntest)
}
#save(all.test.id, file="alltestID.RData")
```


## Seven Procedures

Run the seven procedures and save the results in `MSPE`.

```{r}
test.id = all.test.id[,1] 

MSPE = rep(0, 7)
names(MSPE) = c("Full", "R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
```

### Full Model

```{r}
full.model = lm(Y ~ ., data = myData[-test.id,])
Ytest.pred = predict(full.model, newdata = myData[test.id,])
MSPE[1] = mean((myData$Y[test.id] - Ytest.pred)^2)
```

```{r}
full_model = function(data, test.id){
  full.model = lm(Y ~ ., data = myData[-test.id,])
  Ytest.pred = predict(full.model, newdata = myData[test.id,])
  mean((myData$Y[test.id] - Ytest.pred)^2)
}
```


### Ridge

Ridge with lambda.min and lambda.1se

```{r}
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[2] = mean((Y[test.id] - Ytest.pred)^2)
```

The output below is zero, which means that `best.lam` is the smallest lambda value in the default lambda sequence used by `cv.glmnet`. If so, we need to include some lambda values that are smaller than `best.lam` in order to see the U shape CV plot. 

```{r}
sum(cv.out$lambda < best.lam)
plot(cv.out)
```

Based on the CV plot, we provide a new lambda sequence as follows.

```{r}
mylasso.lambda.seq = exp(seq(-4, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, 
                   lambda = mylasso.lambda.seq)
plot(cv.out)
```

Based on the CV plot, we have to keep decreasing lambda values. Eventually, we use the following lambda sequence. 


```{r}
mylasso.lambda.seq = exp(seq(-10, -2, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, 
                   lambda = mylasso.lambda.seq)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[2] = mean((Y[test.id] - Ytest.pred)^2)

best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[3] = mean((Y[test.id] - Ytest.pred)^2)
```

```{r}
ridge_regression = function(X, Y, lambda.seq, test.id){
  mylasso.lambda.seq = lambda.seq
  cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, 
                     lambda = mylasso.lambda.seq)
  best.lam = cv.out$lambda.min
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  R_min = mean((Y[test.id] - Ytest.pred)^2)
  
  best.lam = cv.out$lambda.1se
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  R_1se = mean((Y[test.id] - Ytest.pred)^2)
  return(c(R_min, R_1se))
}
```


In your submission, **choose your lambda sequence based on one or two training-and-test splits, and then stick to that sequence for all 50 iterations.**

**Choosing lambda sequence**

We take two random T and ensure that the lambda sequence is reasonable for these splits, for use in the remainder of the iterations.
We find that for T = 2, a range -12 to 1 captures the minimum of the curve:
```{r}
test.id = all.test.id[,2]
test.lambda.seq = exp(seq(-12, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = test.lambda.seq)
sum(cv.out$lambda < cv.out$lambda.min)
```

Taking another example, T = 9, we find that -12 to 1 also captures the minimum, so we decide to use this range for our lambda sequence. 
```{r}
test.id = all.test.id[,9]
test.lambda.seq = exp(seq(-12, 1, length.out = 100))
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 0, lambda = test.lambda.seq)
plot(cv.out)

ridge.lambda.seq = test.lambda.seq
```


### Lasso

Lasso with lambda.min, lambda.1se, and Refit

```{r}
cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
best.lam = cv.out$lambda.min
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[4] = mean((Y[test.id] - Ytest.pred)^2)

best.lam = cv.out$lambda.1se
Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
MSPE[5] = mean((Y[test.id] - Ytest.pred)^2)

mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
MSPE[6] = mean((Ytest.pred - Y[test.id])^2)
```


The CV plot indicates that the default lambda sequence for Lasso seems fine. 


```{r}
plot(cv.out)
```


```{r}
lasso_regression = function(X, Y, test.id){
  cv.out = cv.glmnet(X[-test.id, ], Y[-test.id], alpha = 1)
  best.lam = cv.out$lambda.min
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  L_min = mean((Y[test.id] - Ytest.pred)^2)
  
  best.lam = cv.out$lambda.1se
  Ytest.pred = predict(cv.out, s = best.lam, newx = X[test.id, ])
  L_1se = mean((Y[test.id] - Ytest.pred)^2)
  
  mylasso.coef = predict(cv.out, s = best.lam, type = "coefficients")
  var.sel = row.names(mylasso.coef)[which(mylasso.coef != 0)[-1]]
  mylasso.refit = lm(Y ~ ., myData[-test.id, c("Y", var.sel)])
  Ytest.pred = predict(mylasso.refit, newdata = myData[test.id, ])
  L_Refit = mean((Ytest.pred - Y[test.id])^2)
  
  return(c(L_min, L_1se, L_Refit))
}
```


### PCR

The principle components regression command, `pcr`, returns both the CV errors and the adjusted CV errors. For the definition of adjusted CV used in `pcr`, check Sec 2.4 of [this paper](https://mevik.net/work/publications/MSEP_estimates.pdf). 

```{r}
mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV")
CVerr = RMSEP(mypcr)$val[1, , ]
adjCVerr = RMSEP(mypcr)$val[2, , ]
best.ncomp = which.min(CVerr) - 1 
best.ncomp

if (best.ncomp==0) {
    Ytest.pred = mean(myData$Y[-test.id])
  } else {
    Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
  }

MSPE[7] = mean((Ytest.pred - myData$Y[test.id])^2)
```

Note that we have to subtract one from `which.min(CVerr)` since the 1st column of the CV table corresponds to the CV error with zero components (i.e., the model with just the intercept) and the k-th column of the CV table corresponds to (k-1) components. 

The prediction function does not seem to work when `best.ncomp = 0`. So we have to handle that case separately.

```{r}
princomp_regression = function(myData, test.id){
  mypcr = pcr(Y ~ ., data= myData[-test.id, ], validation="CV")
  CVerr = RMSEP(mypcr)$val[1, , ]
  adjCVerr = RMSEP(mypcr)$val[2, , ]
  best.ncomp = which.min(CVerr) - 1 
  
  if (best.ncomp==0) {
      Ytest.pred = mean(myData$Y[-test.id])
    } else {
      Ytest.pred = predict(mypcr, myData[test.id,], ncomp=best.ncomp)
    }

  return(mean((Ytest.pred - myData$Y[test.id])^2))
}
```


```{r}
MSPE
```



## Simulation
### BostonData2
```{r}
MSPE = matrix(nrow = T, ncol = 7)
colnames(MSPE) = c("Full", "R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
for(t in 1:T){
  test.id = all.test.id[,t] 
  MSPE[t, 1] = full_model(myData, test.id)
  MSPE[t, 2:3] = ridge_regression(X, Y, ridge.lambda.seq, test.id)
  MSPE[t, 4:6] = lasso_regression(X, Y, test.id)
  MSPE[t, 7] = princomp_regression(myData, test.id)
}
```


```{r}
boxplot(MSPE)
```

### BostonData3

```{r}
myData = read.csv("BostonData3.csv")
myData = myData[, -1]
dim(myData)
X = data.matrix(myData[,-1])  
Y = myData[,1]  
```

```{r}
MSPE = matrix(nrow = T, ncol = 6)
colnames(MSPE) = c("R_min", "R_1se", "L_min", "L_1se", "L_Refit", "PCR")
for(t in 1:T){
  test.id = all.test.id[,t] 
  MSPE[t, 1:2] = ridge_regression(X, Y, ridge.lambda.seq, test.id)
  MSPE[t, 3:5] = lasso_regression(X, Y, test.id)
  MSPE[t, 6] = princomp_regression(myData, test.id)
}
```

```{r}
boxplot(MSPE)
```


### Discussion
Some points to discuss:

* Expectation before running the study and why
    +  For BostonData2, 78 predictors are quadratic terms of the 12 numerical predictors, and pairwise interactions between the 12 numerical predictors. As such, some of the features may not have predictive power, or have a high correlation with other predictors. The same is true for BostonData3; especially so considering the inclusion of 500 noisy features, which we know have no predictive power.
    + We would expect that the full model would perform the worst since the full model contains all features. Including all features (given what we know about them) could result in overfitting of the training data and lower performance on the testing data. 
    + From 'An Introduction to Statistical Learning with Applications in R' we "might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size." 
    + It is difficult, therefore, to make a prediction on BostonData2 of the performance of Ridge vs Lasso because we don't know which coefficients are small or zero a priori. However, for BostonData3 we can expect that Lasso regression will perform much better because we know that 500 features have a coefficient of zero.
* Do results agree with expectations
    + The full model had worse performance than some of the other models, but it did not perform as poorly as expected in comparison to the other models.
    + Our expectation that Lasso regression performs better than Ridge regression for the BostonData3 dataset held true. 
* Do Ridge and PCR perform similarly?
    + We would expect Ridge and PCR to perform similarly as both models penalize the features in which multicollinearity occurs. Per 'Elements of Statistical Learning': "Ridge regression shrinks the regression coefficients of the principal components... Principal componenet regression truncates them." In practice, Ridge performs better than PCR.We see this is the case for BostonData3, though they have similar performance for BostonData2. 
* Changes in rank/gap from BostonData2 to BostonData3
    + We see a drop in performance for the results between BostonData2 and BostonData3. This makes sense, as additional noise has been introduced that impacts the regression models. We would expect that this additional noise would lead to worse performance as some variables may have spurious correlations in the train data which could lead to the feature coefficients not being sufficiently penalized or removed by the regression models, which would then lead to worse performance on the testing data. This impact is especially profound for those models which can only penalize coefficients (such as Ridge) but not remove them (such as Lasso). 
* Lasso performance across simulations
    + Lasso can zero out noise features, and achieves similar performance on BostonData2 and BostonData3. The performance on BostonData3 is slightly worse, which is to be expected since so much additional noise was introduced, but the performance did not decrease as significantly as it did for Ridge and PCR. 